{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"../resources/cropped-SummerWorkshop_Header.png\">  \n",
    "\n",
    "<h1 align=\"center\">Allen Decoding Tutorial SWDB 2017 </h1> \n",
    "<h3 align=\"center\">Monday, August 28, 2017</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>This notebook will introduce you to the supervised learning (decoding) using the scikit-learn package. Here we have data from different groups (classes), and we aim to use features of that data to predict which class data were drawn from.\n",
    "\n",
    "<p>We will start by simulating data from two Gaussian distributions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import some stuff to get started\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Task 1:**  Define the two distributions we will draw simulate data from. These are the ``classes''.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu1 = np.array([2., 5.])\n",
    "mu2 = np.array([3., 4.])\n",
    "\n",
    "cov1 = np.array([[1., -.5], [-.5, 1.]])\n",
    "cov2 = np.array([[5., 1.], [1., 5.]])\n",
    "\n",
    "cov1inv = np.linalg.inv(cov1)\n",
    "cov2inv = np.linalg.inv(cov2)\n",
    "\n",
    "print(cov1)\n",
    "print(cov2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Task 2:**  Visualize the distributions of the data from each class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Nmesh = 100\n",
    "x_plot = np.linspace(-10., 10., Nmesh)\n",
    "gauss1 = np.zeros((Nmesh, Nmesh))\n",
    "gauss2 = np.zeros((Nmesh, Nmesh))\n",
    "\n",
    "for i in range(Nmesh):\n",
    "    for j in range(Nmesh):\n",
    "        Xplot = np.array([x_plot[i], x_plot[j]])\n",
    "        gauss1[i, j] = np.exp(-.5*(Xplot - mu1).dot(cov1inv).dot((Xplot - mu1).T))\n",
    "        gauss2[i, j] = np.exp(-.5*(Xplot - mu2).dot(cov2inv).dot((Xplot - mu2).T))\n",
    "\n",
    "gauss1 = gauss1 / np.sqrt(2 * np.pi * np.linalg.det(cov1))\n",
    "gauss2 = gauss2 / np.sqrt(2 * np.pi * np.linalg.det(cov2))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(gauss1, extent=(-10, 10, 10, -10)); \n",
    "ax[0].set_title('Class 1')\n",
    "ax[0].set_xlabel('Feature 1')\n",
    "ax[0].set_ylabel('Feature 2')\n",
    "\n",
    "ax[1].imshow(gauss2, extent=(-10, 10, 10, -10)); \n",
    "ax[1].set_title('Class 2')\n",
    "ax[1].set_xlabel('Feature 1')\n",
    "ax[1].set_ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 1:**  Sample 1000 data points, 500 from each class. Place them in in a single array. Hint: check np.random for sampling routines. It will be convenient later if you arrange your data array to alternate elements of the two classes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 2:**  Create class labels for your data: a vector where each element labels the corresponding element of your data array.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p> We have the advantage of knowing that our simulated data are from Gaussian distributions, so we will begin Gaussian discriminant analysis (aka quadratic discriminant analysis). This classifier takes into account that the classes are defined by Gaussian distributions, each with their own mean and covariance matrix. The likelihood ratio test for whether a sample was drawn from one class or the other then yields a quadratic discrimination boundary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "QDA = QuadraticDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Task 3**  What methods does the classifier have?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 3:**  Train (fit) your classifier! To start with, use just 10 data points to train with. For the gaussian discriminant, ``training\" means choosing data to use for training, and computing the sample means and covariances. You could do this by hand, but scikit-learn's decoders provide a handy method. Print the classifier's score on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 4:**  Write a function to visualize the decision boundary for your trained classifier. To do this, visualize the feature space, and predict the label for each point in it. Hint: check out np.meshgrid in order to create data points that span a space, and then try flattening and concatenating the results to feed to the classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 5:**  How does the decision surface of the classifier evolve as you add training points? Add a single training point at a time and plot the decision surface from ~4 to ~10 training points. On each plot, also show the training data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF;\">\n",
    "\n",
    "<p>In order to avoid over-fitting, we should evaluate our classifier on different data than we used to train it. This is called cross-validation. If we have a data set of K points, we can hold out a fraction of our data to use as testing data. The classifier's ability to generalize shows us how it can learn the class distributions, rather than the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "# check sklearn.cross_validation if your version doesn't have cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Task 4**  What methods do scikit-learn's KFold and LeaveOneOut have? What other types of cross-validation scheme live in sklearn.model_selection?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(KFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 6:**  Reserve one-fifth of your data to use as testing data. Train the classifier with your train data and evaluate on the test data. Then repeat so that each fifth of the data gets used as a separate test set. What are the training and test scores on each fold?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 7:**  Train your classifier with increasing amounts of training data, and test it on held-out data. How do the training and test performances depend on the number of training points? The training and test scores as a function of number of training points are called a learning curve. Hint: check sklearn.learning_curve or sklearn.model_selection for useful functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>Here we've given ourselves lots and lots of very low-dimensional data. This is often not the situation we are in; we often have high-dimensional data in limited amounts. Let's see how our classifier performs in this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Task 5**  Generate a new, high-dimensional data set from two classes of Gaussian data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "mu1 = np.random.rand(N)\n",
    "mu2 = np.random.rand(N)\n",
    "\n",
    "cov1 = np.random.randn(100, 100)\n",
    "cov1 = cov1.dot(cov1.T)\n",
    "\n",
    "cov2 = np.random.randn(100, 100)\n",
    "cov2 = cov2.dot(cov2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_total = 1000\n",
    "X1 = np.random.multivariate_normal(mu1, cov1, size=num_total/2)\n",
    "X2 = np.random.multivariate_normal(mu2, cov2, size=num_total/2)\n",
    "XX = np.zeros((num_total, N))\n",
    "XX[::2] = X1\n",
    "XX[1::2] = X2\n",
    "\n",
    "YY = np.zeros((num_total))\n",
    "YY[::2] = 0.\n",
    "YY[1::2] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 8:**  Plot the learning curve for the quadratic discriminant on the high-dimensional dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>The quadratic discriminant has so many parameters that for small enough amounts of data, it can yield perfect performance on training data but score poorly on test data. If we do not have enough data to train our classifier well, we can instead try to reduce the number of its parameters; to regularize it. \n",
    "\n",
    "<p>For the quadratic discriminant, one way to do this is to make additional assumptions about the form of the data. For example, we could assume that our two classes have different means but the same covariance matrix. This yields Linear Discriminant Analysis (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 9:**  Compute the learning curve for the linear discriminant. Compare it to the quadratic discriminant, for both low- and high-dimensional data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> Instead of assuming that our different classes share a covariance matrix, we could instead assume that the different features are uncorrelated within each class. If we assume that the classes all share a covariance matrix, this gives us LDA with a diagonal covariance. Otherwise, we obtain the ``Naive Bayes'' classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 10:**  Compute the learning curve for the Naive Bayes classifier for both the low- and high-dimensional data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> Above we used decoders that relied on the assumption that the data were drawn from Gaussian distributions. If this is not the case, we can still use these decoders but they might not perform as expected. They are examples of parametric decoders: they fit a parameterized distribution (here, Gaussians) to the training data and that distribution determines the test classifications.\n",
    "\n",
    "<p> There are also non-parametric decoders, that do not rely on fitting a parametric model for the data. Perhaps the simplest such is the K-Neighbors classifier. This classifies test points based on majority vote among the labels of the K nearest training points.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Task 6:**  What methods and parameters does the K Nearest Neighbors classifier have?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 11:**  Compare learning curves for the K Nearest Neighbors classifier using 5 neighbors and 100 neighbors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> How do we choose how many neighbors to use? We don't want to use our test data - that defeats the purpose of it. But if we use just the training data, then we risk overfitting within each training fold.\n",
    "\n",
    "<p> A principled way is to use nested cross-validation. By nested cross-validation. Within each training fold, we will do another round of cross-validation to choose how many neighbors to use. Ie, we will split each outer training fold into an inner training fold and inner validation fold. On each outer training fold, we will do a round of cross-validation where we split that training fold into inner training and inner validation sets, evaluate each potential parameter value on them, and choose the parameter value with the best inner validation score. We then test the classifier, with that parameter value, on the outer test fold. \n",
    "\n",
    "<p> So each training fold will have its own value for n_neighbors. Some variation between these hyperparameters across training folds is natural, but if they are wildly different something's probably going on.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p>**Exercise 12**: Train your K-Neighbors classifier using 5-fold cross-validation. In each training fold, perform a second, inner round of 5-fold cross-validation with different numbers of neighbors. For the outer training fold, choose the number of neighbors with the best validation score from the inner cross-validation; use it to compute the test score. Hint: check sklearn.model_selection.GridSearchCV for a handy function to use in the inner loop.\n",
    "\n",
    "<p> Do this for the low-dimensional dataset and the high-dimensional dataset. How many neighbors are optimal for each?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Further exercises</h2>\n",
    "<p>Here are some further exercises:\n",
    "\n",
    "<li> Compare the KFold and StratifiedKFold cross-validation schemes.\n",
    "<li> Compare the Naive Bayesian and diagonal LDA decoders with low and high-dimensional data. To implement diagonal LDA, check the parameters of the classifier.\n",
    "<li> Implement scikit-learn's Logistic Regression. It has a parameter that weights regularization of its trained weights. How should you choose that parameter? Hint: what did we do for K Neighbors?\n",
    "<li> Compare the performance of Logistic Regression and a Support Vector Machine on the simulated low- and high-dimensional data.\n",
    "<li> In the examples above, we have equal amounts of data from each class. If we don't, what should we measure chance performance relative to? Try shuffling the class labels and training the different classifiers on shuffled data.\n",
    "<li> Use neural responses from the brain observatory to predict which stimulus was on the screen.\n",
    "<li> Use information about anatomical projections from a brain region to predict projections to it, or vice versa.\n",
    "<li> Use information about electrophysiological response properties of different cre lines to predict the genetic identity of test cells.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
